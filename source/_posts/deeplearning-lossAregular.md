---
title: "机器学习: 损失函数与正则化"
date: 2025-07-06
categories: [ML/DL]
tags: [DL, ML, Regularization]
mathjax: true
---
<!-- placeholder -->
<!-- more -->
## 损失函数及正则化

### 损失函数概念及约定

- 损失：表示模型预测值与真实值之间差距的实数，越接近零表示误差越小

  代价：是整个数据集上**所有样本的损失的平均值**，即损失的期望，但事实上代价通常也被称作损失，大多数教材并不明确地区分它们

  实际训练中，求和并除以$n$使损失变为代价，使其可比较样本数量不同的数据集，并根据整个样本集的代价进行更优秀的调整

- 损失函数：输入为模型的预测值，输出为损失的一种函数

- 在一次训练以后，模型会通过损失函数估计出损失，并通过某种提前设定好的优化方法调整参数(如权重、偏置等)

  无论使用的优化方法是否为梯度下降法，损失函数都是机器学习算法中很重要的一部分

- 约定$y$为真实值、$\hat y$为模型的预测值

- 一个损失函数适用于某种问题总是可以由**最大似然估计**推导

### 回归任务

- 回归任务：输出为一系列**确切的数值**，目标是预测某个**连续型变量**的分布

- 均方误差(`Mean Squared Error`，`MSE`)，又称`L2 Loss`：$\begin{align}L_{MSE}=\frac1n\sum_{i=1}^n(y_i-\hat y_i)^2\end{align}$

  - 将真实值和预测值的差表示出来：$z=y-\hat y$，且假设误差$z\sim N(0,\sigma^2)$

    则真实值在已知输入的条件下，$y\ |\ X\sim N(\hat y,\sigma^2)$

    根据极大似然估计法，多条样本的极大似然函数为$\begin{align}L=\prod_{i=1}^n\frac1{\sqrt{2\pi\sigma^2}}e^{\Large-\frac{(y_i-\hat y_i)^2}{2\sigma^2}}\end{align}$

    取对数后为$\begin{align}-\frac n2\ln(2\pi\sigma^2)-\frac1{2\sigma^2}\sum_{i=1}^n(y_i-\hat y_i)^2\end{align}$，而极大化似然函数等价于极小化均方误差

  - 均方误差的性质：连续光滑、处处可导，是凸函数可最小化，梯度简单且稳定(与误差线性相关)，因此能够快速收敛但对离群点较敏感

- 平均绝对误差(`Mean Absolute Error`，`MAE`)，又称`L1 Loss`：$\begin{align}L_{MSE}=\frac1n\sum_{i=1}^n|y_i-\hat y_i|\end{align}$

  - 平均绝对误差的性质：连续、仅在$y=0$处不可导，所有点的梯度相同，因此收敛速度较稳定，对离群点具有更强的鲁棒性

- 均方根误差(`Root MSE`，`RMSE`)：$L_{RMSE}=\sqrt{L_{MSE}}$

### 分类任务

- 分类任务：输出为一系列**离散的值**(表示不同的类别，可编号)，目标是预测某个**离散型变量**的分布
  
  - 二分类任务：将输入分为两个类别，一个输入仅属于一个类别
  
    真实值标签为**非零即一的值**
  
    模型输出一个$(0,1)$的实数，若大于一定程度则认为属于正类，否则属于反类
  
  - 单标签多分类任务：将输入分为$k$个类别，一个输入仅属于一个类别
  
    真实值标签为**一次性向量**(`one-hot`向量)，仅有一个分量为$1$，其余均为$0$
  
    模型输出多个$(0,1)$的实数，取最大者为预测的类别
  
  - 多标签多分类任务：将输入分为$k$个类别，一个输入可属于多个类别
  
    真实值标签为**多热向量**，允许多个分量为$1$
  
    模型输出多个$(0,1)$的实数，根据不同策略预测出多个类别
- 交叉熵损失(`Cross-Entropy Loss`)：基础为信息论中的交叉熵，在不同任务中由于真实值所遵循的分布不尽相同，其具体式子有所不同
  
  - 信息论中的交叉熵：$\begin{align}-\sum_iP(y_i)\ln Q(y_i)\end{align}$
  
    信息熵表示某个概率分布的不确定性，越小其分布越稳定，离散随机变量的真实分布$P$的熵为$\begin{align}H(P)=-\sum_iP(y_i)\ln P(y_i)\end{align}$
  
    交叉熵则是估计分布$Q$在近似真实分布$P$时的平均信息量，$\begin{align}H(P,Q)=-\sum_iP(y_i)\ln Q(y_i)\end{align}$
  
    交叉熵总大于信息熵，因为$Q$总是不能完美近似$P$，交叉熵和信息熵的差值被称为**相对熵**，也称$KL$散度，记为$\begin{align}D_{KL}(P\ ||\ Q)=\sum_iP(y_i)\ln(\frac{P(y_i)}{Q(y_i)})\end{align}$
  
    在机器学习中将$P$视为固定分布，即$H(P)$为常量，因此最小化$D_{KL}$等价于**最小化交叉熵**
  
  - 二分类任务：真实分布$P$服从$01$分布
  
    因此最大化似然函数等价于最小化$\begin{align}L_{CE}=-\frac1n\sum_{i=1}^n[y_i\ln\hat y_i+(1-y_i)\ln(1-\hat y_i)]\end{align}$
  
    此交叉熵又称二进制交叉熵，即`BCE`损失函数
  
  - 单标签多分类任务：真实分布$P$服从**分类分布**
  
    最大化似然函数等价于最小化$\begin{align}L_{CE}=-\frac1n\sum_{i=1}^n\sum_ky_{ik}\ln\hat y_{ik}\end{align}$
  
    若已知真实值为第$j$项，则$\begin{align}L_{CE}=-\frac1n\sum_{i=1}^n\ln\hat y_{ij}\end{align}$
  
  - 多标签多分类任务：真实分布$P$服从二项分布，等价于对每个类别进行二分类交叉熵的求和
  
    $\begin{align}L_{CE}=-\frac1n\sum_{i=1}^n\sum_k[y_{ik}\ln\hat y_{ik}+(1-y_{ik})\ln(1-\hat y_{ik})]\end{align}$
- `Hinge`损失函数：
- 余弦相似度损失函数：
- 指数损失函数：

### 聚类任务

- 聚类任务：与分类任务相似，但聚类任务依据分析数据本身特点而非标签来分组，实际是将整个样本集分为多个具有相似特征的样本的集合
- `SSE`损失函数：

### 其它损失函数

- 一些更复杂的任务：

  - 结构化输出：
    - 转录任务：输入为一系列**非结构化**的数据，输出为离散的**有结构的文本标签**
    - 机器翻译任务：输入和输出均为有结构的**符号序列**，两种序列使用的语言不同
  - 异常检测：从输入中标记出异常、非典型的个体
  - 缺失值填补：补充样本中的已知缺失，其输出通常作为某种机器学习算法的输入
  - 去噪：去除样本中损坏的部分，这些损坏通常是非预期发生的
- `KL`散度损失函数：

### 正则化技术

- 正则化(`Regularization`)：是为了**防止过拟合**的一系列技术，其原理是主动限制模型的复杂度

  传统的正则化通过对损失值添加正则项的方式限制模型的复杂度，其原理是**最大后验估计**

  限制模型复杂度，一般是故意地让某些神经元结点“死亡”，即它们的权重为零

- 常见的正则化：

  - `L1`正则化(例如`Lasso`回归)：正则项为$\begin{align}\lambda\sum_i|w_i|\end{align}$，能使得部分过小的权重为零，只有少数的权重显著不为零

  - `L2`正则化(例如`Ridge`回归)：正则项为$\begin{align}\lambda\sum_iw_i^2\end{align}$，使权重均匀减小，但不至于减小至零

  - `Dropout`：在训练过程中`dropout`会**随机**关闭部分神经元结点，这被实践证明是一种有效的`trick`

    直观的理解是：这种`dropout`使每次训练涉及的部分是在所有的可能子网络中的一个，相当于每次训练的神经网络结构均不同，使得模型不会依赖某个局部特征

    其具体做法是：在一次训练中，每次经过一个神经元前，以$p$概率使该神经元**临时死亡**(由伯努利分布提供采样)，为了使输出期望与原来一致，使有效输出乘以$\begin{align}\frac{1}{1-p}\end{align}$

    因此，反向传播时，只会更新那些未死亡神经元的参数

    测试时，保留所有神经元，正常地进行前向传播

    在实现中，通常会把`Dropout`视为和一般神经网络层同等级别的“层”，称为`Dropout`层

  - 早停(`Early Stopping`)：在验证集性能无法提升时，提前结束训练

    模型如果在一定次数内(称为耐心值)，验证指标改进幅度均小于最小改进阈值，则提前结束训练，并恢复到效果最优的版本输出最终模型
