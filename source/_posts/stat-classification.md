---
title: "统计学习: 分类任务"
date: 2025-11-18
categories: [SE Courses, stat]
tags: [stat]
mathjax: true
---
<!-- placeholder -->
<!-- more -->
# 分类任务

## 逻辑回归

### 二分类逻辑回归

- 针对分类任务，通常可用概率值估计类别，例如二分类求出一个`0~1`之间的概率，若概率大于某个阈值则取其中一类

  而一般的线性回归会求出超出`0~1`范围的数，这是一种不合理
- 如同之前分析的线性回归问题“数据非线性”的解决方法那样，逻辑回归通过对预测变量作变换来解决分类任务
  
  一元逻辑回归的形式是：

  $$
  p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}=\frac{1}{1+e^{-\beta_0-\beta_1X}}
  $$

  这在人工智能里称为激活函数`Sigmoid`，其中可将$\beta_0+\beta_1X$替换为多元线性回归的形式来拟合多元分类任务
- 逻辑回归的想法就是将分类任务转换为回归任务

  计算估计参数的方法自然是极大似然估计法，处理定性变量和一般线性回归相同，通过设计哑变量来表示

  预测变量和响应变量的相关性通过`Z`统计量及其假设检验计算
- 和线性回归一样，如果预测变量之间高相关，那么容易产生混淆现象

### 多分类逻辑回归

- 和线性回归不同，针对逻辑回归，“多元”一般指多分类，即输出是多个类别
- 使用的激活函数是`Softmax`，`Sigmoid`是`Softmax`的特例：假设输出类别数为`K`，那么第`i`个输出是

  $$
  Softmax(X_i)=\frac{e^{z_i}}{\sum_{k=1}^Ke^{z_k}}
  $$

## 线性判别分析

- `LDA`（线性判别分析）是另一种间接估计$Pr(Y=k|X=x)$的方法，通过建立预测变量的分布模型，根据贝叶斯定理估计后验概率，最后分类
- 贝叶斯定理：

  $$
  Pr(Y=y|X=x)=\frac{\pi_yf_y(x)}{\sum_(l=1)^K\pi_lf_l(x)}
  其中\pi_y=Pr(Y=y),f_y(x)=Pr(X=x|Y=y)
  $$

- 估计$Pr(X=x|Y=y)$是难题，假设它服从正态分布，那么代入可得贝叶斯分类器，经过推导得到观测的最终分类将是使下式得到最大值的第$k$类：

  $$
  \delta_y(x)=x\frac{\mu_y}{\sigma^2}-\frac{\mu_y^2}{2\sigma^2}+\log(\pi_y)
  $$

- 线性判别分析和逻辑回归的区别：
  - 逻辑回归是参数模型，假设这个条件分布是变换后的线性模型，通过拟合$Pr(Y=y|X=x)$的模型并计算参数得到计算式子
  - `LDA`是非参数模型，假设这个条件分布服从正态分布，通过贝叶斯估计直接推导得出的计算式子，然后求解决定性因子$\delta_y(x)$得到$y$
- 何时使用`LDA`
  - 不同类别区分度较大时
  - 样本量较少无法计算参数时
  - 响应分类类别数较大
  - 通用：假设服从正态分布时和逻辑回归类似，但可以假设服从其它各种分布
- 决策边界：`LDA`会出现对于不同的`y`其`\delta_y(x)`相同的情况，即：

  $$
  \delta_{y_1}(x)=\frac{\mu_{y_1}}{\sigma^2}-\frac{\mu_{y_1}^2}{2\sigma^2}+\log(\pi_{y_1})=\delta_{y_2}(x)=\frac{\mu_{y_2}}{\sigma^2}-\frac{\mu_{y_2}^2}{2\sigma^2}+\log(\pi_{y_2})
  $$

  由等式计算得到的$x$，就是决策边界

- 先验概率对决策边界的影响：

  根据等式，先验概率$\pi_y$越大的类别，其覆盖的`x`范围越大，即决策边界会“远离”该类别

- $p=1$时`LDA`的假设：不同类别的`x`服从方差$\sigma^2_y$相同的正态分布
  
  $p>1$时`LDA`的假设：不同类别的`x`服从协方差矩阵相同的多元正态分布，允许不同预测变量之间有相关性（协方差可非零）

## 混淆矩阵

- 横坐标为预测分类、纵坐标为真实分类

  $$
  \begin{align}&(真|假)(阳|阴)性率=\frac{预测成功|预测失败\times预测阳性|预测阴性}{真实阳性|真实阴性}\\&真阳性率=\frac{预测阳且真实阳}{真实阳}\\&预测(阳|阴)性率=\frac{预测成功(阳|阴)}{预测(阳|阴)}\end{align}
  $$

## 二次判别分析

- `QDA`(二次判别分析)假设每一类别的预测变量仍服从多元高斯分布，但允许不同类的预测变量的协方差矩阵不同，在这样的假设下估计出来的$\sum_k$不同，代入后得到：

  $$
  \delta_y(x)=-\frac12(x-\mu_y)^T\sum_y^{-1}(x-\mu_y)+\log\pi_y-\frac12\log|\sum_y|
  $$

- `QDA`要求计算等同于类别数个的协方差矩阵，带给他更高的光滑度
- 由于判别函数是二次的，`QDA`的光滑度更高，因此偏差更小方差更大，当**数据量非常大**时，`LDA`的假设十分远离实际，因此偏差异常的大，而`QDA`带来的方差变大并不那么重要，反而能减小偏差，此时`QDA`更适合
