---
title: "统计学习: 变量选择"
date: 2025-11-20
categories: [SE Courses, stat]
tags: [stat, Regularization]
mathjax: true
---
<!-- placeholder -->
<!-- more -->
# 变量选择

## 子集选择法

- 最优子集选择法：对`p`个预测变量，进行总共$\sum_{i=1}^pC_{p}^i=2^p-1$次拟合，即会拟合所有可能的模型

  具体做法是先对每一个`i`，求出包含`i`个变量的最优模型(训练误差最小)，然后在剩下的`p`个最优模型里再选择最优(真正进行交叉验证，选择泛化误差最小)

  但效率极低，当`p`较大时$2^p$级别的复杂度就会使拟合无法完成
- 逐步选择：
  - 逐步向前选择：从不包含任何变量的模型开始，遍历没有加入模型的预测变量，选择使得下一模型的训练误差最小的变量加入，直到所有变量全部加入

    模型仍然能获得`p`个“最优”模型，在其中估计泛化误差选择最小者

    其拟合次数为$\sum_{i=1}^p=\frac{p(p+1)}{2}$，约为$p^2$
  - 逐步向后选择：从包含全部变量的模型开始，遍历所有已经加入模型的预测变量，选择使得下一模型的训练误差最小的变量删除，直到所有变量全部删除

    模型仍然能获得`p`个“最优”模型，在其中估计泛化误差选择最小者

    其拟合次数为$\sum_{i=1}^p=\frac{p(p+1)}{2}$，约为$p^2$

    向后选择在$p>n$时无法进行，因为方程组无解(方程数小于未知变量数)
- 最优模型的选择依据
  - 模型选择依赖于交叉验证估计的较为准确的泛化误差

## 压缩估计

- 压缩估计是一种针对训练过程的变量选择方法，通过影响损失函数来间接影响拟合函数，让某些多余变量的系数接近零或变成零，因此也可作为变量选择的一节
- 压缩估计不是通过对训练误差直接增加惩罚项或直接估计测试误差，而是试图在训练时就对最小化函数增加惩罚项，一般是针对系数加惩罚，来减小系数的方差以测试误差的方差最终有效地防止过拟合
- 前面介绍的方法都没有影响基于最小二乘法的训练过程，而是通过各种手段估计测试误差，以其作为依据选择最优模型，而压缩估计则是直接修改了损失函数的形式，从而在选择最优模型之前的那一步减弱模型的过拟合

### `Lasso`回归(`L1`回归)

- `Lasso`回归是在最小二乘法的基础上，将最小化函数$RSS$改造为：

  $$
  Lasso=\sum_{i=1}^n\left(y_i-\hat y_i\right)^2+\lambda\sum_{j=1}^p|\beta_j|
  $$

- 最小化|\beta_j|，它的解在解空间里将有“尖角”特性，其和$RSS$最小化的解的交集将有很大概率落在坐标轴上，因此$\beta_j$系数更容易被压缩到零，从而更能实现变量选择的效果
- 与第一节的权衡一致，增加惩罚项使偏差增大，方差减小，能减小模型的过拟合现象
- 它的另一个形式是作为一个约束(其中$s$为无穷大时等价于最小二乘法)：

  $$
  \sum_{j=1}^p|\beta_j|\le s
  $$

### `Ridge`回归(`L2`回归)

- 岭回归是在最小二乘法的基础上添加惩罚项$\lambda\sum_{j=1}^p\beta_j^2$，它的效果并没有`Lasso`回归那么好，因为其解是圆滑的，随着系数会接近零但不会为零，这减小了可解释性
- 与`Lasso`回归类似，它的另一个形式是作为一个约束：

  $$
  \sum_{j=1}^p\beta_j^2\le s
  $$

### 选择$\lamdba$或$s$

- 和子集选择法类似，最终仍然通过交叉验证来估计测试误差，只不过针对的是不同$\lambda$下的最优模型，即不是直接比较不同变量组合的模型，而是比较不同正则化强度的模型

## 降维选择法

- 此前的子集选择法会完全不使用部分变量，而压缩估计则是通过压缩系数来间接地选择变量，而降维选择法则是通过将原始$p$个变量**变换**为更少的$M$个变量(并不是将一些变量合为一组来进行训练)
- 降维选择法直接影响拟合函数的形式，也是一种针对训练过程的变量选择
- 变量个数$M$也通过交叉验证来选择

#### 主成分回归

- `PCR`回归通过投影方法，把$p$个变量投影到$M$个变量，通过乘投影矩阵得到主成分

#### 偏最小二乘

- `PLS`偏最小二乘可以视作有指导的主成分回归，响应变量$Y$会参与降维的过程
