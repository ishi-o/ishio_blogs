---
title: "统计学习: 线性回归"
date: 2025-11-17
categories: [SE Courses, stat]
tags: [stat]
mathjax: true
---
<!-- placeholder -->
<!-- more -->
# 线性回归

## 简单线性回归

### 最小二乘法

- 线性回归是监督学习模型，假设所有预测变量和响应变量具有线性关系
- 最小二乘回归(`OLS`)是最常用的简单线性回归方法，它采用的损失函数是残差平方和，其中残差是预测值与真实值的差值，残差平方和(`RSS`)为$\begin{align}\sum_{i=1}^n(\hat y_i-y_i)^2\end{align}$
- 最小化损失函数是几乎所有方法使用的估计系数方法，对于`OLS`而言则是最小化`RSS`，即求导使导数为零然后解方程

### 模型参数估计值指标

- 随机误差：随机误差是线性回归本就假设进去的一项，它的方差`$\sigma^2$`是不可约误差，是未知的常数
- 标准差`SE`：表示参数估计结果偏离真实值的平均值，用到了随机误差的方差以及`RSS`
- 残差标准差(`RSE`)：虽然不可知，但可以估计，一般用`t(n-2)`统计量来估计
- 置信区间：xx%置信区间通过参数的估计值加上一定比例的标准差来得到，例如95%置信区间的范围是$参数估计值\pm 2SE(参数估计值)$
- 预测区间
- 参数估计值是否为零：通过对`t`统计量进行假设检验，若`p`值接近零，认为有显著影响，因此否定估计值为零的原假设

### 模型预测值评价指标

- 残差标准差表示模型预测值偏离真实值的平均值
- `$R^2$`统计量通过总平方和以及残差平方和来计算，总平方和是真实值与平均值的差的平方和，该统计量越接近`1`，说明模型能更好的解释变异
- 偏差-方差权衡：模型的光滑度代表了它的复杂程度，光滑度越高，模型的非线性拟合能力越强，方差越小，但偏差也会越大

  随光滑度增高，训练误差单调递减，但测试误差随之呈`U`型变化，左侧呈现出欠拟合问题(模型复杂程度太低)，右侧呈现出过拟合问题(训练误差小于不可约误差，但测试误差较大)

## 多元线性回归

- 多元线性回归假设各个预测变量不相关
- 多元线性回归同样使用最小二乘法估计系数
- 多元线性回归同样通过`t`统计量的`p`值判断某个特定系数是否为零，此外还通过`F`统计量的`p`值判断是否所有预测变量的系数均为零

## 处理定性的预测变量

- 二值变量使用一个哑变量即可，即是某类时赋值为`1`，否则为零
- `n`值变量使用`n-1`个哑变量，所有哑变量均为零时表示剩下那个状态

## 处理线性回归的假设

- 多元线性回归假设各个预测变量不相关，但现实中几乎不可能，为了减小相关性影响，引入交互项(乘积项)，和一般的预测变量相同地参与模型评价
- 实验分层原则：若交互项存在，那么参与交互项的预测变量(称为主效应项)都应在模型中

## 线性回归的问题

- 数据的非线性：画出残差-拟合值图像，对于每一个拟合值，求出的残差的均值应是零，若存在若干点残差均值远离零，那么很有可能有数据的非线性问题

  解决办法是观察图像并猜测有可能的线性关系，添加对**预测变量作一定变换**的非线性项
- 随机误差项自相关：原本假设是$n$个$\varepsilon$之间互不相关(且假设服从正态分布)，如果相关会导致估计标准差低于真实标准差，链式导致置信区间变窄

  此问题常出现在时间序列数据上，解决办法是增加采样时间或注重随机采样
- 随机误差项方差非恒定：画出残差-拟合值图像，随拟合值变化，残差的均值虽然接近零，但上下范围随之变化

  解决办法是对**响应变量作一定变换**
- 离群点：响应值标签是异常的，针对一元或二元线性回归，可以画出真实值-输入值图像或**学生化残差图**来发现离群点

  学生化残差指残差和残差标准差的比值，提供一个易于分析的评估范围(绝对值大于`3`视为离群点)

  离群点不会过多影响最小二乘计算出的参数估计值，但会影响评价指标(因为`RSE`会被影响)，如`p`值、置信区间
- 高杠杆点：输入数据(预测变量)的值是异常的(简单理解为附近没有其它样本点)，可以通过计算杠杆统计量值量化寻找

  高杠杆点和离群点不同，它会极大影响最小二乘计算出的参数估计值，必须找出它
- 数据的共线性：预测变量之间高度相关，难以猜测哪些变量是真正相关的，若两个高度相关的变量放在一起，参数的标准差增大、`t`统计量值减小、假设检验的`p`值增大，导致无法看出哪个变量是真正和响应变量相关的，且难以解释系数

  可通过计算协方差矩阵或计算方差膨胀因子(`VIF`)发现共线性数据

  可通过其它回归方法或去除某些变量解决该问题
